# Заметки по теории вероятностей из курса Райгородского Андрея Михайловича на площадке Sirus
https://edu.sirius.online/#/course/2168

## Основы комбинаторики
### Размещения и сочетания
Пусть есть множество из n элементов.

- _Число размещений без повторений_ $A_{n}^{k}$ — количество способов расположить на k пронумерованных позициях элементы множества так, чтобы никакие два не повторялись.
- _Число размещений с повторениями_ $\bar{A}_{n}^{k}$ — количество способов расположить на k пронумерованных позициях элементы множества (на разных позициях элементы могут повторяться).
- _Число сочетаний без повторений_ $C_{n}^{k}$ — количество способов выбрать набор из k элементов, где элементы не повторяются.
- _Число сочетаний с повторениями_ $\bar{C}_{n}^{k}$ — количество способов выбрать набор из k элементов, где элементы могут повторяться.

Если исходное множество — это алфавит (то есть n = 33), то указанные величины соответствуют следующему:

- $A_{n}^{k}$ равно количеству слов длины k, состоящих из различных букв;
- $\bar{A}_{n}^{k}$ равно количеству слов длины k, состоящих из необязательно различных букв;
- $C_{n}^{k}$ равно количеству наборов из k различных букв (то есть наборов букв без учёта порядка);
- $\bar{C}_{n}^{k}$ равно количеству наборов из k необязательно различных букв (то есть наборов букв без учёта порядка).

### Размещения и сочетания. Формулы
Через $n!$ обозначается произведение натуральных чисел от 1 до n. Удобно считать, что $0!=1$. Тогда формулы для числа размещений и числа сочетаний можно записать в следующем виде:

- $A_{n}^{k}=\frac{n!}{(n−k)!}=n⋅(n−1)⋅…⋅(n−k+1)$;
- $\bar{A}_{n}^{k}=n^k$;
- $C_{n}^{k}=\frac{n!}{k!⋅(n−k)!}=\frac{n⋅(n−1)⋅…⋅(n−k+1)}{k}!$;
- $\bar{C}_{n}^{k}=C_{n+k−1}^{k}$ (без доказательства).

Количество _перестановок_ чисел от 1 до n (то есть количество способов записать эти числа в ряд в каком-нибудь порядке) равно $n!$.

### Бином Ньютона
**Бином Ньютона.** При натуральном n выполнено равенство

$(x+y)^n=C_{n}^{0}⋅x^n+C_{n}^{1}⋅x^{n−1}y+C_{n}^{2}⋅x^{n−2}y^2+…+C_{n}^{k}⋅x^{n−k}y^k+…+C_{n}^{n}⋅y^n$.

В честь бинома числа сочетаний $C_{n}^{k}$ также называют _биномиальными коэффициентами_.

Для доказательства заметим, что в левой части стоит произведение n скобок. Слагаемое $x^{n−k}y^k$ получается, если выбрать y из k скобок и x из оставшихся n−k скобок. Количество способов сделать это как раз равно $C_{n}^{k}$.

### Свойства биномиальных коэффициентов
**Свойства биномиальных коэффициентов**

- $C_{n}^{k}=C_{n}^{n-k}$. Это свойство можно легко увидеть из явной формулы, однако оно легко следует и из комбинаторных соображений: выбрать k объектов из n — это то же самое, что не выбрать n−k объектов из n.
- $C_{n}^{k}=C_{n-1}^{k}+C_{n-1}^{k-1}$. Это свойство также можно доказать как алгебраически, так и комбинаторно. Рассмотрим множество из n элементов и выделим в нём один элемент. Тогда все способы выбрать k элементов из n можно разбить на две части: выделенный элемент не выбран или выбран. В первом случае количество вариантов равно $C_{n-1}^{k}$, а во втором — $C_{n-1}^{k-1}$.
- $C_{n}^{0}+C_{n}^{1}+…+C_{n}^{n}=2^n$. Алгебраически это тождество следует из бинома, в который подставили единицы:
    
    $C_{n}^{0}+C_{n}^{1}+…+C_{n}^{n}=(1+1)^n=2^n$.
    
    Комбинаторно это тождество следует из того, что и левая, и правая части равны количеству последовательностей из n цифр, каждая из которых равна 0 или 1.

## Классическое определение вероятности
### Пример с игральным кубиком
Рассмотрим подбрасывание идеального игрального кубика. Обозначим возможные результаты через $\omega_{1},\omega_{2},...,\omega_{6}$, где $\omega_{1}$ соответствует выпаданию единицы, $\omega_{2}$ — выпаданию двойки и так далее.

Мы считаем, что:

- $\omega_{1},\omega_{2},...,\omega_{6}$ исчерпывают все возможные исходы;
- никакие два исхода из $\omega_{1},\omega_{2},...,\omega_{6}$ не могут произойти одновременно;
- исходы $\omega_{1},\omega_{2},...,\omega_{6}$ равновероятны.

В таких предположениях естественно задать вероятности каждого из исходов как $P(\omega_{i})=\frac{1}{6}$ для i от 1 до 6.

### Определение
Рассмотрим эксперимент, в результате которого могут реализоваться конечное число n _элементарных исходов_ (_элементарных событий_) $\omega_{1},\omega_{2},...,\omega_{n}$ таких, что

- $\omega_{1},\omega_{2},...,\omega_{n}$ исчерпывают все возможные результаты эксперимента;
- никакие два исхода из $\omega_{1},\omega_{2},...,\omega_{n}$ не могут произойти одновременно;
- исходы $\omega_{1},\omega_{2},...,\omega_{n}$ равновероятны.

Тогда естественно задать вероятность каждого из исходов как $P(\omega_{i})=\frac{1}{n}$.

_Пример._ Рассмотрим результат перетасовки колоды из 36 игральных карт. Элементарными исходами будут все возможные порядки карт в колоде. Всего 36 карт можно расположить 36! способами, а значит, вероятность любого отдельного порядка карт в колоде $P(\omega_{i})=\frac{1}{36!}$.

### Событие и его вероятность
_Событие_ — это некоторое множество элементарных исходов. Событие, не содержащее ни одного элементарного исхода (то есть пустое множество, $\emptyset$), называется _невозможным событием_. Событие, состоящее из всех элементарных исходов, называется _достоверным событием_.

При классическом задании вероятностей (то есть при выполнении трёх условий, описанных в предыдущей лекции) для произвольного события A его вероятность $P(A)$. естественно задать как $P(A)=\frac{|A|}{n}$., где $|A|$ — количество элементов в множестве A.

В частности, $P(\emptyset)=0$ и $P(\{\omega_{1},\omega_{2},...,\omega_{n}\})=1$.

### Свойства вероятности
Если у эксперимента есть n элементарных исходов, то количество соответствующих ему событий равно $2^n$.

Поскольку события — это множества, то с ними можно выполнять теоретико-множественные операции.

- _Объединение_ $A \cup B$ событий A и B — это событие, отвечающее тому, что произошло событие A или событие B (не исключая того, что могли произойти оба). Иначе говоря, реализовался элементарный исход, благоприятствующий либо только A, либо только B, либо и A, и B.
- _Пересечение_ $A \cap B$ событий A и B — это событие, отвечающее тому, что произошло и событие A, и событие B. Иначе говоря, реализовался элементарный исход, благоприятствующий и A, и B.
- _Отрицание_ $\bar{A}$ события A — это событие, отвечающее тому, что событие A не произошло, то есть не реализовался ни один из элементарных исходов, благоприятствующих A. На языке теории множеств это можно записать как $\bar{A}=\{\omega_{1},\omega_{2},...,\omega_{n}\}$ \ $A$.

Достоверное событие, состоящее из всех элементарных исходов, обозначается $\Omega=\{\omega_{1},\omega_{2},...,\omega_{n}\}$

**Свойства вероятности при классическом определении**

- $P(\emptyset)=0, P(\Omega)=1$.
- Если $A \notin \{\emptyset, \Omega\}$, то $P(A) \in (0, 1)$.
- $P(A)=1 - P(\bar{A})$.
- $P(A \cup B)=P(A)+P(B)-P(A \cap B)$.

## Условная вероятность и независимость событий
### Условная вероятность
Допустим, есть пространство элементарных исходов $\Omega=\{\omega_{1},\omega_{2},...,\omega_{n}\}$ и некоторое непустое событие $B ⊆ \Omega$ (то есть $P(B)>0$), про которое известно, что оно произошло. Зададимся вопросом, чему в этом случае равна вероятность некоторого события A.

_Условной вероятностью A при условии_ B называется вероятность того, что событие A произойдёт, если известно, что событие B произошло. Условная вероятность определяется формулой
						$P(A|B)=\frac{|A \cap B|}{|B|}=\frac{P(A \cap B)}{P(B)}$.

Из этого определения непосредственно следует _теорема умножения:_
						$P(A \cap B)=P(A|B)⋅P(B)$.

### Независимость двух событий
События A и B называются _независимыми_, если $P(A \cap B)=P(A)⋅P(B)$. При этом допускается, что вероятность A или B может быть нулевой.

Когда одно из событий, например B, имеет положительную вероятность, условие независимости эквивалентно более интуитивному условию $P(A|B)=P(A)$, которое показывает, что если A не зависит от B, то реализация B не меняет вероятности A.

События A и B называются _несовместными_, если $A \cap B=\emptyset$. Важно отметить, что независимость и несовместность — это разные понятия. В частности, если два события несовместны и имеют ненулевые вероятности, то они обязательно будут зависимыми.

### Независимость нескольких событий
События $A_{1},...,A_{k}$  называются _независимыми_ (как ещё часто говорят, _независимы в совокупности_, или _взаимно независимы_), если для любого набора из этих событий верно, что вероятность пересечения всех событий в наборе равна произведению вероятностей событий в наборе.

Если говорить более формально, то $A_{1},...,A_{k}$ независимы, если для любого набора несовпадающих индексов $1⩽i_{1},…,i_{m}⩽k$ верно равенство
						$P(A_{i_{1}} \cap ... \cap A_{i_{m}})=P(A_{i_{1}})⋅…⋅P(A_{i_{m}})$.

### Пример
Если события попарно независимы, то из этого не следует, что они независимы в совокупности. Традиционный пример событий, независимых попарно, но не в совокупности, строится с помощью кубика в виде тетраэдра. Пусть три грани тетраэдра покрашены в красный, жёлтый и зёленый цвета, а на оставшейся грани есть все три цвета: и красный, и жёлтый, и зёленый. Рассмотрим три события: на выпавшей (нижней) грани тетраэдра есть красный, жёлтый и зелёный цвета. Эти три события не будут независимы, но при этом любые два из них независимы.


## Формула полной вероятности и формула Байеса
### Формула полной вероятности
**Формула полной вероятности.** Пусть множество элементарных исходов Ω разбито на непересекающиеся подмножества $B_{1},...,B_{m}$ такие, что все они имеют ненулевую вероятность. Тогда для произвольного события A его вероятность может быть вычислена по формуле
						$P(A)=P(A|B_{1})⋅P(B_{1})+...+P(A|B_{m})⋅P(B_{m})$.

Иногда под этой формулой понимают более простое аналогичное утверждение
						$P(A)=P(A \cap B_{1})+...+P(A \cap B_{m})$.

### Формула полной вероятности. Пример
_Пример._ Даны две урны. В первой находятся 3 белых и 2 чёрных шара, а во второй — 2 белых и 2 чёрных. Из первой урны наугад достаётся шар и кладётся внутрь второй урны. После этого шары во второй урне тщательно перемешивают и наугад достают из неё 2 шара. Найдите вероятность того, что оба этих шара белые.

### Формула Байеса
**Формула Байеса.** Пусть множество элементарных исходов $\Omega$ разбито на подмножества $B_{1},...,B_{m}$  такие, что они все они имеют ненулевую вероятность. Тогда для произвольного события A, имеющего ненулевую вероятность, вероятность $B_{i}$ при условии A может быть вычислена по формуле

				$P(B_{i}|A)=\frac{P(A|B_{i})⋅P(B_{i})}{P(A)}=\frac{P(A|B_{i})⋅P(B_{i})}{P(A|B_{1})⋅P(B_{1})+...+P(A|B_{m})⋅P(B_{m})}$

### Вывод формулы Байеса через определение условной вероятности
Напомним, что $P(A|B)=\frac{|A \cap B|}{|B|}=\frac{P(A \cap B)}{P(B)}$. Откуда следует, что $P(A \cap B)=P(A|B)⋅P(B)$.
Заметим, что $P(A \cap B)=P(B \cap A)=P(B|A)⋅P(A)$. Отсюда следует следующее тождество:
						$P(A \cap B)=P(B \cap A)$
						$P(A|B)⋅P(B)=P(B|A)⋅P(A)$
						$P(B|A)=\frac{P(A|B)⋅P(B)}{P(A)}$
где $P(A)$ можно посчитать по формуле полной вероятности.

### Формула Байеса. Пример
_Пример._ Студент решает задачу с $k$ вариантами ответа. Если студент знает, как решать задачу, он даёт правильный ответ. Если нет — может угадать его с вероятностью $\frac{1}{k}$. Профессор знает, что вероятность того, что студент знает, как решать задачу, равна $p$. Студент дал правильный ответ. Найдите вероятность того, что студент дал правильный ответ, зная решение задачи, а не угадав его.


## Схема испытаний Бернулли
### Элементарные исходы
Схема испытаний Бернулли описывает последовательность $n$ независимых испытаний, каждое из которых может заканчиваться успехом или неудачей. Типичный пример — последовательность из $n$ подбрасываний несимметричной монеты, где выпадание решки можно условно считать успехом, а выпадание орла — неудачей.

Вероятность успеха в одном испытании традиционно обозначается $p$, а вероятность неудачи часто обозначается $q=1−p$.

Элементарным исходом в данной схеме является последовательность из $n$ нулей и единиц. Если на i-й позиции в этой последовательности стоит 1, то в i-м испытании результатом был успех, если стоит 0, то результатом была неудача.

### Вероятность элементарного исхода
$\Sigma_{i=1}^{n}$ 
